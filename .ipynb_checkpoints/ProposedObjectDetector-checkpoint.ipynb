{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "print('Python ' + sys.version)\n",
    "print('numpy: ' + np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset from keras\n",
    "### Keras includes special support for loading some of the benchmark datasets. One is the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you never had used CIFAR-10 before with keras, it will download the dataset first. The next time you resort to using the dataset again, it will use the downloaded copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the dataset\n",
    "#### Displaying the first 9 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 9):\n",
    "    plt.subplot(3, 3, 1+i)\n",
    "    plt.imshow(X_train[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling is a must in Deep Learning networks\n",
    "#### Scaling the dataset features to have values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the 1st image before feature scaling so that it can be later\\\n",
    "# used for sanity checks\n",
    "X_train_0 = X_train[0]\n",
    "\n",
    "# Feature scaling\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding \n",
    "#### The labels in the dataset are numerical values which inherently are ordinal. This ordinality can skew the weights, for eg, a label 6 will skew the weights of the ANN more towards itself than the other lower values, ie, 0, 1, 2, 3, 4... ANN's work in such a way that this happens and may not create a nice model. So we one-hot encode the labels into a vector of 0's and 1's. The length of each of such vector will be equal to the number of classes and each vector will have a 1 at only one position. For eg, if the categories are dog, cat and lizard, the one-hot encoding will have a vector for each example, each of such vector will be of length 3 (since number of classes = 3). If an example has label 'cat', then the one-hot encoded vector \"may be\" [1,0,0], if an example has label 'dog', the one-hot encoded vector \"may be\" [0,1,0] and for a 'lizard' it \"may be\" [0,0,1]. I have explicitly mentioned \"may be\" because the position of 1 in the vector for each category depends on the underlying implementation of the one-hot encoding done, ie, what I have mentioned is according to an implemention that assigns one-hot encoded vectors based on lexicographical order, other implementation may assign these vectors differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "\n",
    "# printing one-hot encoded vector for 0th label (which is 6)\n",
    "print(y_train[0])\n",
    "\n",
    "# from cifar-10 dataset webpage, we can see that label 6 is for frog\n",
    "# checking if X_train[0] is a frog's image\n",
    "plt.subplot(3,3,1) # doing this to plot image in a smaller grid so that\\\n",
    "                # the 32x32 image looks a bit clearer\n",
    "plt.imshow(X_train_0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It does look like a frog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the All-CNN model for comparison\n",
    "#### Reference used-  All-CNN paper (https://arxiv.org/abs/1412.6806)\n",
    "\n",
    "Model - C (taken from the paper, that uses the Max Pool layers)\n",
    "* Input 32 × 32 RGB image\n",
    "* 3 × 3 conv. 96 ReLU\n",
    "* 3 × 3 conv. 96 ReLU\n",
    "* 3 × 3 max-pooling stride 2\n",
    "* 3 × 3 conv. 192 ReLU\n",
    "* 3 × 3 conv. 192 ReLU\n",
    "* 3 × 3 max-pooling stride 2\n",
    "* 3 × 3 conv. 192 ReLU\n",
    "* 1 × 1 conv. 192 ReLU\n",
    "* 1 × 1 conv. 10 ReLU\n",
    "* global averaging over 6 × 6 spatial dimensions\n",
    "* 10 or 100-way softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries to build layers of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout # used for regularisation to prevent\\\n",
    "                                # overfitting which is common with ANNs\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the All-CNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allcnn_proposed(weights = None):\n",
    "    # Initialising the ANN as a sequence of layers\n",
    "    classifier = Sequential()\n",
    "    \n",
    "    # Adding convolution layers\n",
    "    # In Convolution2D the activation parameter is the activation function that is applied after convolution is\\\n",
    "    # done, ie, activation is applied on the feature map obtained. This is done by a separate activation layer.\n",
    "    # I have considered the convolution + activation as one big step of two stages- convolution and then\\\n",
    "    # activation. If you would like to consider these two as two separate steps, then keras allows you to implement\\\n",
    "    # your code that way as well. This can be done as follows:\n",
    "    # classifier.add(Convolution2D(96, 3, 3))\n",
    "    # classifer.add(Activation('relu'))\n",
    "    # For the above two line two work, you will have to import Activation class from keras.layers package\n",
    "    \n",
    "    classifier.add(Convolution2D(96, (3, 3), input_shape = (32, 32, 3), padding = 'same', activation = 'relu'))\n",
    "    classifier.add(Convolution2D(96, (3, 3), padding = 'same', activation = 'relu'))\n",
    "    classifier.add(Convolution2D(96, (3, 3), padding = 'same', activation = 'relu', strides = 2))\n",
    "    \n",
    "    classifier.add(Convolution2D(192, (3, 3), padding = 'same', activation = 'relu'))\n",
    "    classifier.add(Convolution2D(192, (3, 3), padding = 'same', activation = 'relu'))\n",
    "    classifier.add(MaxPooling2D(pool_size = (3,3), strides = 2))\n",
    "    \n",
    "    classifier.add(Convolution2D(192, (3, 3), padding = 'same', activation = 'relu'))\n",
    "    \n",
    "    classifier.add(Convolution2D(192, (1, 1), padding = 'valid', activation = 'relu'))\n",
    "    \n",
    "    classifier.add(Convolution2D(10, (1, 1), padding = 'valid', activation = 'relu'))\n",
    "    \n",
    "    classifier.add(GlobalAveragePooling2D())\n",
    "    classifier.add(Activation('softmax'))\n",
    "    \n",
    "    if weights:\n",
    "        classifier.load_weights(weights)\n",
    "    \n",
    "    return classifier\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = allcnn_traditional()\n",
    "# adam is a stochastic gradient descent algorithm\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "print(classifier.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 350, batch_size = 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
